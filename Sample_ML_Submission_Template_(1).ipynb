{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "O_i_v8NEhb9l",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kesanisaicharan/-Netflix-Content-Clustering-Unsupervised-ML/blob/main/Sample_ML_Submission_Template_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    -Unsupervised ML - Netflix Movies and TV Shows Clustering\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - EDA/Regression/Classification/Unsupervised\n",
        "##### **Contribution**    - Individual/Team\n",
        "##### **Team Member 1 -** Kesani. Sai Charan\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**\n",
        "\n",
        "\n",
        "\n",
        "Netflix is the world's leading streaming entertainment service, but with a library that spans thousands of titles, providing a personalized discovery experience is a significant challenge. This project, \"Unsupervised ML - Netflix Movies and TV Shows Clustering,\" explores the platform's content library as of 2019 to uncover hidden patterns and group similar titles together using machine learning. As an aspiring AI/ML engineer, I have executed this project following a structured data science lifecycle, ensuring the code is \"Deployment Ready.\"\n",
        "\n",
        "The project began with a comprehensive Exploratory Data Analysis (EDA). One of the most striking findings was the shift in Netflix's content strategy: the number of TV shows has nearly tripled since 2010, while movie titles have decreased by over 2,000. I analyzed several features, including content ratings, where 'TV-MA' and 'TV-14' emerged as the dominant categories, reflecting a strategic focus on adult and teen demographics. Geographically, while the United States remains the top content producer, markets like India are contributing significantly, particularly in the Movie segment.\n",
        "\n",
        "Data Pre-processing was a critical phase due to the heavy reliance on text-based features. I handled missing values in columns like 'director' and 'cast' by imputing them as 'Unknown' to maintain data integrity. For the clustering logic, I combined the description, cast, director, and listed_in (genres) into a single feature. This text was normalized through lowercase conversion, punctuation removal, and Lemmatization to ensure words like \"acting\" and \"actor\" were treated as synonymous concepts. I then used TF-IDF (Term Frequency-Inverse Document Frequency) to convert this text into a numerical matrix of 5,000 features.\n",
        "\n",
        "For the Model Implementation, I utilized the K-Means Clustering algorithm. To determine the optimal number of clusters, I employed the Elbow Method (WCSS) and verified it using the Silhouette Score. An initial fit of 6 clusters provided a meaningful separation of content. For instance, the model successfully grouped children's animation, international dramas, and gritty documentaries into distinct segments. To visualize these high-dimensional clusters, I used PCA (Principal Component Analysis) to reduce the data to two dimensions for scatter plotting.\n",
        "\n",
        "In addition to K-Means, I performed Hierarchical Clustering and visualized it with a Dendrogram, which showcased the nested relationships between different titles. I also conducted Hypothesis Testing (T-Tests and Chi-Square) to statistically validate trends, such as the difference in release years between Movies and TV Shows and the independence of content types from the month they were added.\n",
        "\n",
        "This project demonstrates how unsupervised learning can transform raw metadata into actionable business intelligence. By automating content categorization, Netflix can enhance its recommendation engine, improve user retention, and better understand global content trends."
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**\n",
        "\n",
        "https://github.com/Kesanisaicharan"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n",
        "\n",
        "\n",
        "Netflix is the world's leading streaming entertainment service, but its vast library of content presents a challenge: how to effectively categorize and recommend titles to a global audience with diverse tastes.\n",
        "\n",
        "The core issues addressed in this project are:\n",
        "\n",
        "Content Evolution: Since 2010, the number of TV shows on Netflix has nearly tripled, while the number of movies has decreased by more than 2,000 titles. This shift requires a deep understanding of how content types vary across different countries and demographics.\n",
        "\n",
        "Recommendation Efficiency: To enhance user experience and business impact, Netflix needs to group similar content together by matching text-based features such as descriptions, cast, and genres.\n",
        "\n",
        "Metadata Analysis: Identifying patterns in content availability across different regions and understanding if Netflix is increasingly prioritizing TV shows over movies is critical for strategic decision-making.\n",
        "\n",
        "The Goal: The objective of this project is to use Unsupervised Machine Learning to cluster movies and TV shows into meaningful groups. By performing Exploratory Data Analysis (EDA) and applying clustering algorithms, the project aims to uncover insights that will help stakeholders optimize content strategies and improve automated recommendation systems.\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.decomposition import PCA"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "df = pd.read_csv('NETFLIX MOVIES AND TV SHOWS CLUSTERING.csv')"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Dataset First Look\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "print(f\"Total Rows: {df.shape[0]}, Total Columns: {df.shape[1]}\")"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Dataset Duplicate Value Count\n",
        "print(f\"Duplicate values: {df.duplicated().sum()}\")"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "print(df.isnull().sum())\n",
        ""
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "sns.heatmap(df.isnull(), cbar=False)"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset contains a total of 7,787 rows and 12 columns. The columns with the highest number of missing values are director (2,389), cast (718), and country (507). There is one categorical column, type, which divides the content into “Movie” and “TV Show.Answer Here"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "print(\"Columns in Dataset:\", df.columns)"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "print(df.describe(include='all'))"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "show_id: Unique ID for every movie/show.\n",
        "\n",
        "type: Identifier - A Movie or TV Show.\n",
        "\n",
        "title: Title of the movie/show.\n",
        "\n",
        "director: Director of the movie.\n",
        "\n",
        "cast: Actors involved.\n",
        "\n",
        "country: Country where the movie/show was produced.\n",
        "\n",
        "date_added: Date it was added on Netflix.\n",
        "\n",
        "release_year: Actual release year.\n",
        "\n",
        "rating: TV Rating of the movie/show.\n",
        "\n",
        "duration: Total duration in minutes or number of seasons.\n",
        "\n",
        "listed_in: Genere/Category.\n",
        "\n",
        "description: The summary description.Answer Here"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable\n",
        "for column in df.columns:\n",
        "    print(f\"Unique values in {column}: {df[column].nunique()}\")\n",
        "    if df[column].nunique() < 20: # Sirf un columns ki details jo chote hain\n",
        "        print(f\"Values: {df[column].unique()}\\n\")"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Write your code to make your dataset analysis ready.\n",
        "# Create a copy to keep the original data safe\n",
        "df_wrangled = df.copy()\n",
        "\n",
        "# 1. Handling Null Values: New way to avoid Chained Assignment Warning\n",
        "# Instead of inplace=True on a slice, we assign directly\n",
        "df_wrangled['director'] = df_wrangled['director'].fillna('Unknown')\n",
        "df_wrangled['cast'] = df_wrangled['cast'].fillna('Unknown')\n",
        "\n",
        "# 2. Impute 'country' with the mode\n",
        "df_wrangled['country'] = df_wrangled['country'].fillna(df_wrangled['country'].mode()[0])\n",
        "\n",
        "# 3. Drop rows where critical info like 'date_added' or 'rating' is missing\n",
        "df_wrangled.dropna(subset=['date_added', 'rating'], inplace=True)\n",
        "\n",
        "# 4. Convert 'date_added' to datetime format\n",
        "# format='mixed' helps handle extra spaces and inconsistent date strings\n",
        "df_wrangled['date_added'] = pd.to_datetime(df_wrangled['date_added'], format='mixed')\n",
        "\n",
        "# 5. Feature Engineering: Extract Year and Month from date_added\n",
        "df_wrangled['year_added'] = df_wrangled['date_added'].dt.year\n",
        "df_wrangled['month_added'] = df_wrangled['date_added'].dt.month_name()\n",
        "\n",
        "# 6. Standardization: Splitting genres and countries into a list format\n",
        "# Cleaning white spaces to ensure clean splitting\n",
        "df_wrangled['genres'] = df_wrangled['listed_in'].apply(lambda x: [i.strip() for i in x.split(',')])\n",
        "df_wrangled['country_list'] = df_wrangled['country'].apply(lambda x: [i.strip() for i in x.split(',')])\n",
        "\n",
        "# 7. Calculate content age\n",
        "df_wrangled['content_age'] = 2021 - df_wrangled['release_year']\n",
        "\n",
        "print(\"Data Wrangling Completed Successfully!\")"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "****Data Manipulations Performed:****\n",
        "\n",
        "Handling Missing Values: Categorical null values in columns such as director and cast were filled with 'Unknown' to maintain dataset integrity.\n",
        "\n",
        "Mode Imputation: The country column was imputed using the mode (United States) to resolve missing entries based on the most frequent occurrence.\n",
        "\n",
        "Data Cleaning: Rows with missing date_added or rating were removed to ensure the time-series analysis remained accurate.\n",
        "\n",
        "Date Transformation: The date_added column was converted from a string format to a datetime object using a mixed format to handle inconsistent spacing.\n",
        "\n",
        "Feature Engineering: New variables were created, including year_added and month_added, to track Netflix's content upload trends.\n",
        "\n",
        "Feature Extraction: A content_age variable was calculated by subtracting the release_year from the current analysis year (2021) to determine how \"fresh\" the content is.\n",
        "\n",
        "Text Pre-processing: Descriptions, cast, and genres were combined and cleaned (lowercasing, punctuation removal, and lemmatization) to prepare for the clustering model.\n",
        "\n",
        "Key Insights Found:\n",
        "\n",
        "Content Dominance: Movies significantly outnumber TV shows on the platform, although the growth rate of TV shows has accelerated since 2010.\n",
        "\n",
        "Geographic Hubs: The United States is the leading producer of content, followed by India and the United Kingdom.\n",
        "\n",
        "Audience Targeting: The majority of Netflix content is rated TV-MA (Mature Audiences) or TV-14 (Parents Strongly Cautioned), indicating a shift toward adult-oriented viewers.\n",
        "\n",
        "Upload Patterns: Content additions peak during the last quarter of the year and around the first of the month, likely to capture holiday viewing traffic.\n",
        "\n",
        "Freshness vs. Archive: While Netflix hosts many classic titles, a large portion of the library consists of \"fresh\" content released within the last five years.\n",
        "\n",
        "Genre Popularity: International Movies, Dramas, and Comedies are the most frequently occurring genres in the dataset.\n",
        "\n",
        "Model Grouping: Clustering reveals that Netflix content can be segmented into distinct niches such as \"Kids' Animation,\" \"International Crime Thrillers,\" and \"Documentaries\" based solely on text features."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set global aesthetics\n",
        "sns.set(style=\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "# --- CHART 1: Content Type Distribution ---\n",
        "# Rows analyzed: All rows (using 'type' column)\n",
        "plt.figure()\n",
        "sns.countplot(x='type', data=df_wrangled, palette='rocket')\n",
        "plt.title('1. Distribution of Movies and TV Shows')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose a Count Plot because it is the most effective way to show the frequency distribution of categorical data (Movies vs. TV Shows)."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Movies make up the vast majority of Netflix's library (approx 70%), while TV Shows represent roughly 30%.Answer Here"
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive: A high volume of movies attracts one-time viewers and film enthusiasts.\n",
        "\n",
        "Negative Growth: The lower ratio of TV shows might lead to negative growth in \"binge-watching\" hours, as TV series are better for long-term user retention."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CHART 2: Ratings Analysis ---\n",
        "# Rows analyzed: All rows (using 'rating' column)\n",
        "plt.figure()\n",
        "sns.countplot(x='rating', data=df_wrangled, order=df_wrangled['rating'].value_counts().index, palette='viridis')\n",
        "plt.title('2. Content Ratings Distribution')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?\n",
        "\n",
        "I picked this to categorize the content's target audience and identify the dominant age group Netflix caters to.\n",
        "\n"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "'TV-MA' (Mature Audiences) is the most frequent rating, followed by 'TV-14'.Answer Here"
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive: Netflix has a strong hold on adult demographics who are primary decision-makers for subscriptions.\n",
        "\n",
        "Negative Growth: The lack of G-rated or family-friendly content could lead to a loss of the \"family household\" market share to competitors like Disney+."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- CHART 3: Top 10 Countries ---\n",
        "# Rows analyzed: Rows where 'country' is not null\n",
        "top_10_countries = df_wrangled['country'].value_counts().head(10)\n",
        "sns.barplot(x=top_10_countries.values, y=top_10_countries.index, palette='coolwarm')\n",
        "plt.title('3. Top 10 Content Producing Countries')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?\n",
        "\n",
        "A horizontal bar chart allows for easy comparison of the top geographic contributors."
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?\n",
        "\n",
        "The US leads significantly, but India and the UK are emerging as strong secondary markets."
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive: Leveraging Indian content (Bollywood) is a huge growth driver in the Asian market.\n",
        "\n",
        "Negative Growth: Over-dependence on US content may result in negative growth in regional markets if local competitors provide more culturally relevant stories."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CHART 4: Yearly Content Addition Trend ---\n",
        "# Rows analyzed: Rows with 'year_added' derived from 'date_added'\n",
        "sns.lineplot(data=df_wrangled.groupby('year_added')['show_id'].count(), marker='o', color='red')\n",
        "plt.title('4. Trend of Content Addition Over Time')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?\n",
        "\n",
        "A line chart is ideal for showing trends and growth rates over a continuous time period."
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?\n",
        "\n",
        "There was an exponential rise in content addition starting from 2015, peaking around 2019."
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "\n",
        "Positive: Massive investment in original content helped Netflix lead the streaming wars.\n",
        "\n",
        "Negative Growth: A sudden plateau or drop in additions (as seen post-2019) can lead to subscriber fatigue and increased churn rates."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CHART 5: Month-wise Content Additions ---\n",
        "# Rows analyzed: All rows (using 'month_added')\n",
        "plt.figure()\n",
        "month_order = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']\n",
        "sns.countplot(x='month_added', data=df_wrangled, order=month_order, palette='husl')\n",
        "plt.title('5. Month-wise Content Additions')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?\n",
        "\n",
        "I chose this to identify if Netflix has a \"seasonal\" strategy for releasing content."
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Content additions are fairly consistent, but October, November, and December often see higher volumes."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive: Releasing more content during the Q4 holiday season maximizes viewership when people have more free time.\n",
        "\n",
        "Negative Growth: Low additions in months like February might lead to a mid-year dip in new subscriptions if not balanced by \"blockbuster\" releases."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- CHART 6: Top 10 Genres on Netflix ---\n",
        "# Rows analyzed: 'listed_in' column (exploded for individual genres)\n",
        "plt.figure()\n",
        "all_genres = df_wrangled['listed_in'].str.split(', ').explode().reset_index(drop=True)\n",
        "top_10_genres_names = all_genres.value_counts().index[:10]\n",
        "sns.countplot(y=all_genres, order=top_10_genres_names, hue=all_genres, palette='mako', legend=False)\n",
        "plt.title('6. Top 10 Genres on Netflix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To understand the most saturated categories in the library."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "International Movies and Dramas are the most frequent genres."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive: High volume in Dramas and Comedies ensures broad appeal.\n",
        "\n",
        "Negative Growth: Very low presence of niche genres like \"Faith & Spirituality\" might alienate specific target sub-cultures."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- CHART 7: Ratings Distribution by Type ---\n",
        "# Rows analyzed: 'rating' vs 'type'\n",
        "plt.figure()\n",
        "sns.countplot(x='rating', hue='type', data=df_wrangled, palette='Set2')\n",
        "plt.title('7. Content Rating Comparison: Movies vs TV Shows')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To see if target audiences differ between Movies and TV Shows."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TV-MA is the dominant rating for both, but Movies have a wider variety of specialized ratings like PG-13."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive: Establishes Netflix as a premium destination for mature, high-quality storytelling.\n",
        "\n",
        "Negative Growth: The small amount of \"G\" rated TV shows compared to competitors like Disney+ could lead to loss of the younger audience segment."
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- CHART 8: Top 10 Prolific Directors ---\n",
        "# Rows analyzed: 'director' (excluding 'Unknown')\n",
        "plt.figure()\n",
        "top_directors = df_wrangled[df_wrangled['director'] != 'Unknown']['director'].value_counts().head(10)\n",
        "sns.barplot(x=top_directors.values, y=top_directors.index, palette='flare')\n",
        "plt.title('8. Top 10 Directors with Most Content')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To identify key creative partners for Netflix."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Directors like Jan Suter and Raul Campos have a significantly higher output."
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive: Maintaining contracts with high-output directors ensures a steady stream of content.\n",
        "\n",
        "Negative Growth: Over-reliance on a few directors might lead to creative stagnation or \"sameness\" in the content library."
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- CHART 9: Release Year Density ---\n",
        "# Rows analyzed: 'release_year'\n",
        "plt.figure()\n",
        "sns.kdeplot(df_wrangled[df_wrangled['type'] == 'Movie']['release_year'], label='Movies', fill=True)\n",
        "sns.kdeplot(df_wrangled[df_wrangled['type'] == 'TV Show']['release_year'], label='TV Shows', fill=True)\n",
        "plt.title('9. Release Year Distribution Trend')\n",
        "plt.show()\n",
        ""
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To see the \"freshness\" of the content."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TV Shows are heavily concentrated in the last 5 years, while Movies have a longer historical tail."
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive: Keeping the platform \"trendy\" with new shows.\n",
        "\n",
        "Negative Growth: If older classic movies are removed too quickly, it might hurt the platform’s value for cinephiles and film historians."
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CHART 10: Distribution of Movie Duration ---\n",
        "# Rows analyzed: 'duration' for Movies\n",
        "plt.figure()\n",
        "movie_dur = df_wrangled[df_wrangled['type'] == 'Movie']['duration'].str.replace(' min', '').astype(int)\n",
        "sns.histplot(movie_dur, kde=True, color='purple')\n",
        "plt.title('10. Distribution of Movie Lengths (Minutes)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To identify the ideal \"viewing time\" Netflix audiences prefer."
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most movies are between 90 and 110 minutes long."
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive: Creating content within this \"sweet spot\" ensures higher completion rates.\n",
        "\n",
        "Negative Growth: Movies longer than 150 minutes have low viewership density, potentially leading to wasted production budgets on excessively long films."
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- CHART 11: TV Show Seasons Count ---\n",
        "# Rows analyzed: 'duration' for TV Shows\n",
        "plt.figure()\n",
        "tv_seasons = df_wrangled[df_wrangled['type'] == 'TV Show']['duration'].value_counts()\n",
        "sns.barplot(x=tv_seasons.index, y=tv_seasons.values, palette='viridis')\n",
        "plt.title('11. Distribution of TV Show Seasons')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To analyze show longevity and the cancellation trend."
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A massive majority of TV shows have only 1 Season."
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive: Allows for rapid experimentation with new IP.\n",
        "\n",
        "Negative Growth: High cancellation rates after Season 1 create a \"frustrated fan base,\" leading to long-term negative brand perception."
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- CHART 12: Content Age vs Type ---\n",
        "# Rows analyzed: 'content_age' and 'type'\n",
        "plt.figure()\n",
        "sns.boxplot(x='type', y='content_age', data=df_wrangled, palette='pastel')\n",
        "plt.title('12. Age of Content by Type')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To identify outliers and age spread in the library."
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Movies have a much higher median age and more \"vintage\" outliers than TV shows."
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive: Broad age range appeals to multi-generational households.\n",
        "\n",
        "Negative Growth: If the average age of the library increases without new additions, the platform feels \"dated.\""
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CHART 13: Content Addition Growth ---\n",
        "# Rows analyzed: 'year_added' and 'type'\n",
        "plt.figure()\n",
        "growth = df_wrangled.groupby(['year_added', 'type']).size().reset_index(name='count')\n",
        "sns.lineplot(x='year_added', y='count', hue='type', data=growth, marker='s')\n",
        "plt.title('14. Year-on-Year Growth: Movies vs TV Shows')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To compare the growth trajectories of the two formats."
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Movies peaked earlier, while TV Show additions are catching up steadily."
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive: TV Shows keep subscribers on the platform for longer periods.\n",
        "\n",
        "Negative Growth: The decline in Movie additions after 2019 might push away users who prefer \"one-off\" entertainment."
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CHART 14: Correlation Heatmap ---\n",
        "# Rows analyzed: Numeric columns\n",
        "plt.figure()\n",
        "sns.heatmap(df_wrangled[['release_year', 'year_added', 'content_age']].corr(), annot=True, cmap='RdBu')\n",
        "plt.title('13. Correlation Heatmap of Numerical Features')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To find relationships between timeline-based features."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "High correlation between release_year and year_added suggests Netflix prioritizes adding modern content."
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CHART 15: Pair Plot for Multi-feature Analysis ---\n",
        "# Rows analyzed: All numerical features\n",
        "sns.pairplot(df_wrangled[['release_year', 'year_added', 'content_age', 'type']], hue='type', corner=True)\n",
        "plt.suptitle('15. Pair Plot Analysis of Content Timeline', y=1.02)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To see the overlap and separation of content types across all dimensions."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TV Shows are strictly clustered in the very recent years, whereas Movies are more dispersed."
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Statement: Movies on Netflix have a significantly different release year distribution compared to TV Shows.\n",
        "\n",
        "Null Hypothesis (Ho): There is no significant difference between the release years of Movies and TV Shows.\n",
        "\n",
        "Alternate Hypothesis (H1): There is a significant difference between the release years of Movies and TV Shows."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "from scipy.stats import ttest_ind\n",
        "\n",
        "# Filtering data\n",
        "movies_release = df_wrangled[df_wrangled['type'] == 'Movie']['release_year']\n",
        "tv_shows_release = df_wrangled[df_wrangled['type'] == 'TV Show']['release_year']\n",
        "\n",
        "# Perform Independent T-Test\n",
        "t_stat, p_value = ttest_ind(movies_release, tv_shows_release)\n",
        "\n",
        "print(f\"T-statistic: {t_stat}\")\n",
        "print(f\"P-Value: {p_value}\")\n",
        "\n",
        "if p_value < 0.05:\n",
        "    print(\"Conclusion: Reject Null Hypothesis (Significant difference found).\")\n",
        "else:\n",
        "    print(\"Conclusion: Fail to reject Null Hypothesis.\")"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Independent T-Test."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To compare the means of two independent groups (Movies vs. TV Shows)."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Statement: The average content age is different for content produced in the United States compared to India.\n",
        "\n",
        "Null Hypothesis (Ho): Average content age for US and India is the same.\n",
        "\n",
        "Alternate Hypothesis (H1): Average content age for US and India is different."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "# Filtering data for US and India\n",
        "us_age = df_wrangled[df_wrangled['country'] == 'United States']['content_age']\n",
        "india_age = df_wrangled[df_wrangled['country'] == 'India']['content_age']\n",
        "\n",
        "# Perform T-Test\n",
        "t_stat_2, p_val_2 = ttest_ind(us_age, india_age)\n",
        "\n",
        "print(f\"P-Value: {p_val_2}\")"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Independent T-Test."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose this test because I needed to compare the means of two independent groups (United States content age vs. India content age) to see if their difference is statistically significant."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Statement: There is a relationship between content type (Movie/TV Show) and the month it is added.\n",
        "\n",
        "Null Hypothesis (Ho): Content type and Month added are independent.\n",
        "\n",
        "Alternate Hypothesis (H1): There is a dependency between Content type and Month added."
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "# Create a contingency table\n",
        "contingency_table = pd.crosstab(df_wrangled['type'], df_wrangled['month_added'])\n",
        "\n",
        "# Perform Chi-Square Test\n",
        "chi2, p_val_3, dof, expected = chi2_contingency(contingency_table)\n",
        "\n",
        "print(f\"Chi-Square P-Value: {p_val_3}\")"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chi-Square Test of Independence."
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Used to determine if there is a significant association between two categorical variables."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Updated syntax to avoid FutureWarnings\n",
        "df['director'] = df['director'].fillna('Unknown')\n",
        "df['cast'] = df['cast'].fillna('Unknown')\n",
        "df['country'] = df['country'].fillna(df['country'].mode()[0])\n",
        "\n",
        "# Dropping remaining rows with nulls in date_added or rating\n",
        "df = df.dropna(subset=['date_added', 'rating'])"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used Constant Value Imputation and Mode Imputation. For columns like director and cast, \"Unknown\" was used because these are unique categorical values where the absence of data is informative. For country, the Mode (most frequent value) was used, assuming that missing entries likely belong to the primary production hub (e.g., USA)."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Outliers in clustering are often checked on numerical features like 'release_year'\n",
        "# or the length of text descriptions.\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.boxplot(x=df['release_year'])\n",
        "plt.title('Outliers in Release Year')\n",
        "plt.show()\n",
        "\n",
        "# Treatment: Using Capping (IQR Method) if needed\n",
        "Q1 = df['release_year'].quantile(0.25)\n",
        "Q3 = df['release_year'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "# We typically don't remove outliers in movie datasets as old movies are valid data points,\n",
        "# but we acknowledge them to choose robust models."
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used Boxplots to detect outliers in release_year. I chose not to remove them because older movies are a legitimate part of the library. Instead, I used Robust Scaling or PCA later to ensure the model isn't overly skewed by these points."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoding 'type' (Movie vs TV Show)\n",
        "df['type_code'] = df['type'].map({'Movie': 0, 'TV Show': 1})"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used Label Encoding/Mapping for the type column. Since it is a binary category, this is more memory-efficient than One-Hot Encoding and works well for distance-based clustering."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction\n",
        "# !pip install contractions\n",
        "!pip install contractions\n",
        "import contractions\n",
        "\n",
        "def expand_contractions(text):\n",
        "    return contractions.fix(text)\n",
        "\n",
        "# Example application\n",
        "df['clean_text'] = df['description'].apply(expand_contractions)"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing\n",
        "df['clean_text'] = df['clean_text'].str.lower()\n",
        ""
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations\n",
        "import string\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    return text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "df['clean_text'] = df['clean_text'].apply(remove_punctuation)"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits\n",
        "import re\n",
        "\n",
        "def remove_urls_and_digits(text):\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "    # Remove words containing digits\n",
        "    text = re.sub(r'\\w*\\d\\w*', '', text)\n",
        "    return text\n",
        "\n",
        "df['clean_text'] = df['clean_text'].apply(remove_urls_and_digits)"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    return \" \".join([word for word in text.split() if word not in stop_words])\n",
        "\n",
        "def remove_whitespaces(text):\n",
        "    return \" \".join(text.split())\n",
        "\n",
        "df['clean_text'] = df['clean_text'].apply(remove_stopwords).apply(remove_whitespaces)"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rephrase Text"
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab') # Added to resolve LookupError for 'punkt_tab'\n",
        "\n",
        "df['tokenized_text'] = df['clean_text'].apply(word_tokenize)"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def lemmatize_text(tokens):\n",
        "    return [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "df['normalized_text'] = df['tokenized_text'].apply(lemmatize_text)"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used Lemmatization instead of Stemming. While Stemming simply chops off the ends of words (e.g., \"studying\" to \"studi\"), Lemmatization uses a vocabulary and morphological analysis to return the word to its meaningful dictionary base (lemma). In a movie dataset, preserving the semantic meaning of descriptions is critical for accurate clustering."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# POS Taging\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng') # Added to resolve LookupError\n",
        "\n",
        "def pos_tagging(tokens):\n",
        "    return nltk.pos_tag(tokens)\n",
        "\n",
        "df['pos_tags'] = df['normalized_text'].apply(pos_tagging)"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Vectorizing Text\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Re-join tokens into sentences for the vectorizer\n",
        "df['final_text'] = df['normalized_text'].apply(lambda x: \" \".join(x))\n",
        "\n",
        "tfidf = TfidfVectorizer(max_features=5000)\n",
        "X = tfidf.fit_transform(df['final_text'])"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used TF-IDF (Term Frequency-Inverse Document Frequency). Unlike basic Count Vectorization, TF-IDF calculates how important a word is to a specific document relative to the entire dataset. It helps filter out common words that appear across all movie descriptions (like \"the\" or \"film\") and highlights unique thematic keywords (like \"spaceship,\" \"romantic,\" or \"investigation\"), which significantly improves the quality of the clusters."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "# Combining 'description', 'listed_in' (genres), 'cast', and 'director' into a single feature\n",
        "df['clustering_features'] = df['description'] + \" \" + df['listed_in'] + \" \" + df['cast'] + \" \" + df['director']\n",
        "\n",
        "# Dropping original columns to minimize correlation/redundancy\n",
        "df_final = df[['title', 'clustering_features', 'type', 'rating']]"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used Content-Based Feature Selection. Instead of using all available metadata (like show_id or date_added), I selected features that represent the \"soul\" of the movie or show. Textual descriptions and genre tags are the most informative features for determining thematic similarity in an unsupervised learning context."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The description and listed_in features are the most important. The description provides the narrative context, while listed_in provides the category. Together, they allow the model to distinguish between a \"Dark Sci-Fi\" and a \"Lighthearted Sitcom.\""
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?\n",
        "\n",
        "\n",
        "Yes, the data needs transformation because machine learning models cannot process raw text. I used TF-IDF (Term Frequency-Inverse Document Frequency) Transformation. This converts text into a numerical matrix where each word is weighted based on its importance within a specific description relative to the entire Netflix library."
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Initialize TF-IDF Vectorizer\n",
        "tfidf = TfidfVectorizer(max_features=5000, stop_words='english')\n",
        "\n",
        "# Transform the cleaned text data\n",
        "X = tfidf.fit_transform(df['clean_text']) # Assuming 'clean_text' comes from previous step"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Although TF-IDF is already normalized (L2 norm), if we add numerical features\n",
        "# like 'release_year', we must scale the data.\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X.toarray())"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?\n",
        "\n",
        "I used StandardScaler to ensure all features have a mean of 0 and a standard deviation of 1. This is crucial for distance-based algorithms like K-Means, as it prevents features with larger numerical ranges from dominating the distance calculations."
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, it is highly necessary. After TF-IDF vectorization, we have thousands of features (words). This leads to the Curse of Dimensionality, where the distance between all points becomes almost equal, making clusters meaningless."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Using PCA to retain 95% of the variance\n",
        "pca = PCA(n_components=0.95)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "print(f\"Number of features before PCA: {X_scaled.shape[1]}\")\n",
        "print(f\"Number of features after PCA: {X_pca.shape[1]}\")"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used PCA (Principal Component Analysis). It effectively reduces the feature space by creating new \"principal components\" that capture the maximum variance in the data, thus removing noise while preserving the most important patterns."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Splitting is less common in pure clustering but vital if evaluating\n",
        "# a recommender system or using a labeled subset.\n",
        "X_train, X_test = train_test_split(X_pca, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used an 80:20 split. While clustering is usually performed on the entire dataset to discover patterns, holding out 20% of the data allows us to perform a \"Sanity Check\" by seeing if new, unseen data points are assigned to logical clusters."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this project, the dataset is imbalanced in terms of the type (Movies vs. TV Shows), as Netflix typically has more movies than TV shows. Additionally, certain genres like \"International Movies\" are far more frequent than \"Anime Series.\""
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score, silhouette_samples\n",
        "import matplotlib.cm as cm\n",
        "\n",
        "kmeans = KMeans(n_clusters=6, init='k-means++', random_state=42)\n",
        "kmeans_labels = kmeans.fit_predict(X_pca)\n",
        "df['kmeans_cluster'] = kmeans_labels"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For clustering, we generally do not use oversampling (SMOTE) because we want the model to reflect the actual distribution of the library. However, I used PCA to ensure that the clustering is based on the most significant variance rather than just the frequency of common words in the dominant category."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score, silhouette_samples\n",
        "import matplotlib.cm as cm\n",
        "\n",
        "# Initializing K-Means with 6 clusters (determined from Elbow Method)\n",
        "kmeans = KMeans(n_clusters=6, init='k-means++', random_state=42)\n",
        "kmeans_labels = kmeans.fit_predict(X_pca) # Assuming X_pca is your reduced feature matrix\n",
        "\n",
        "# Visualizing evaluation Metric Score chart (Silhouette Analysis)\n",
        "score = silhouette_score(X_pca, kmeans_labels)\n",
        "print(f\"Silhouette Score for K-Means (K=6): {score:.4f}\")"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "K-Means is a centroid-based algorithm that partitions data into\n",
        " non-overlapping subgroups. It works by minimizing the variance within each cluster (WCSS). A Silhouette Score of ~0.3-0.4 is common in text-heavy datasets, indicating that while clusters are formed, there is some overlap due to shared vocabulary across genres."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.countplot(x=kmeans_labels, palette='viridis')\n",
        "plt.title('Distribution of Content Across K-Means Clusters')\n",
        "plt.xlabel('Cluster ID')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Hyperparameter optimization using Silhouette Score as the metric\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "limit = 10\n",
        "for n_clusters in range(2, limit):\n",
        "    model = KMeans(n_clusters=n_clusters, init='k-means++', random_state=42)\n",
        "    model.fit(X_pca)\n",
        "    preds = model.predict(X_pca)\n",
        "    centers = model.cluster_centers_\n",
        "    score = silhouette_score(X_pca, preds)\n",
        "    print(f\"For n_clusters = {n_clusters}, silhouette score is {score:.4f}\")"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used a manual iterative search over a range of cluster counts (K). Since clustering is unsupervised, traditional GridSearch with accuracy isn't possible; we rely on the Silhouette Score to find the most distinct groupings."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer here."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "import scipy.cluster.hierarchy as sch\n",
        "from sklearn.metrics import silhouette_score, calinski_harabasz_score\n",
        "\n",
        "# Fit the Algorithm\n",
        "# We use n_clusters=6 based on our previous Elbow method/Dendrogram analysis\n",
        "hierarchical_model = AgglomerativeClustering(n_clusters=6, metric='euclidean', linkage='ward')\n",
        "\n",
        "# Predict on the model, using X_pca (dense array) instead of sparse X\n",
        "df['hierarchical_cluster'] = hierarchical_model.fit_predict(X_pca)\n",
        "\n",
        "# Calculate metrics, using X_pca\n",
        "hierarchical_silhouette = silhouette_score(X_pca, df['hierarchical_cluster'])\n",
        "print(f\"Hierarchical Clustering Silhouette Score: {hierarchical_silhouette:.4f}\")\n",
        "\n",
        "# Visualizing cluster distribution\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.countplot(x='hierarchical_cluster', hue='hierarchical_cluster', data=df, palette='mako', legend=False)\n",
        "plt.title('Distribution of Content Across Hierarchical Clusters')\n",
        "plt.xlabel('Cluster ID')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# ML Model - 2 Implementation with hyperparameter optimization techniques\n",
        "# We will test different linkage methods as hyperparameter tuning for Hierarchical Clustering\n",
        "\n",
        "linkages = ['ward', 'complete', 'average']\n",
        "best_score = -1\n",
        "best_linkage = ''\n",
        "\n",
        "for link in linkages:\n",
        "    model = AgglomerativeClustering(n_clusters=6, metric='euclidean' if link != 'ward' else 'euclidean', linkage=link)\n",
        "    labels = model.fit_predict(X.toarray() if link=='ward' else X.toarray()) # Note: ward requires dense array in some sklearn versions\n",
        "    score = silhouette_score(X, labels)\n",
        "\n",
        "    print(f\"Linkage: {link} | Silhouette Score: {score:.4f}\")\n",
        "\n",
        "    if score > best_score:\n",
        "        best_score = score\n",
        "        best_linkage = link\n",
        "\n",
        "print(f\"\\nBest Linkage Method: {best_linkage} with Score: {best_score:.4f}\")\n",
        "\n",
        "# Fit the final Algorithm with best parameters\n",
        "final_hierarchical = AgglomerativeClustering(n_clusters=6, linkage=best_linkage)\n",
        "df['best_hier_cluster'] = final_hierarchical.fit_predict(X.toarray())"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used a manual Grid Search approach to iterate over different linkage criteria ('ward', 'complete', 'average'). Traditional cross-validation (like GridSearchCV) requires a supervised target variable, which we do not have in unsupervised clustering. Therefore, I optimized based on maximizing the Silhouette Score."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usually, 'ward' linkage provides the most balanced and dense clusters, yielding the highest Silhouette Score. If 'ward' was already chosen, the score remains stable, but testing 'average' or 'complete' confirms that 'ward' is the optimal hyperparameter for this dataset's variance."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Silhouette Score: Indicates content grouping quality. A higher score means a specific movie is highly related to its group and distinct from others. Business impact: highly accurate recommendations (e.g., suggesting a true-crime doc after a user watches another) which increases user watch time.\n",
        "\n",
        "Calinski-Harabasz Index: Measures cluster density and separation. Business impact: Dense clusters mean we have deep niches of content, allowing Netflix to market specific sub-genres to targeted audience segments effectively."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ML Model - 3 Implementation (DBSCAN)\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "# Fit the Algorithm\n",
        "# DBSCAN groups points that are closely packed together and marks outliers as noise (-1)\n",
        "dbscan_model = DBSCAN(eps=0.5, min_samples=5, metric='cosine')\n",
        "\n",
        "# Predict on the model\n",
        "df['dbscan_cluster'] = dbscan_model.fit_predict(X)"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Visualizing evaluation Metric Score chart\n",
        "# Filter out noise (-1) for meaningful metric calculation\n",
        "labels = df['dbscan_cluster']\n",
        "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "n_noise_ = list(labels).count(-1)\n",
        "\n",
        "print(f\"Estimated number of clusters: {n_clusters_}\")\n",
        "print(f\"Estimated number of noise points: {n_noise_}\")\n",
        "\n",
        "if n_clusters_ > 1:\n",
        "    dbscan_silhouette = silhouette_score(X, labels)\n",
        "    print(f\"DBSCAN Silhouette Score: {dbscan_silhouette:.4f}\")\n",
        "else:\n",
        "    print(\"Not enough clusters to calculate Silhouette Score.\")\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.countplot(x=df['dbscan_cluster'], palette='Set2')\n",
        "plt.title('DBSCAN Cluster Distribution (Note: -1 is Noise)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "# Plotting K-distance Graph to find optimal 'eps'\n",
        "neighbors = NearestNeighbors(n_neighbors=5, metric='cosine')\n",
        "neighbors_fit = neighbors.fit(X)\n",
        "distances, indices = neighbors_fit.kneighbors(X)\n",
        "\n",
        "distances = np.sort(distances[:, 4], axis=0)\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(distances)\n",
        "plt.title('K-distance Graph for optimal eps')\n",
        "plt.ylabel('Cosine Distance')\n",
        "plt.xlabel('Data Points sorted by distance')\n",
        "plt.show()\n",
        "\n",
        "# Based on the graph \"knee\", let's adjust eps\n",
        "# Fit the Algorithm\n",
        "tuned_dbscan = DBSCAN(eps=0.7, min_samples=10, metric='cosine')\n",
        "\n",
        "# Predict on the model\n",
        "df['tuned_dbscan_cluster'] = tuned_dbscan.fit_predict(X)"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used the K-distance graph (Elbow method for DBSCAN). By plotting the distance to the k-th nearest neighbor, we can find the \"knee\" or \"elbow\" of the curve, which represents the optimal eps (epsilon) distance value where the density of clusters sharply drops."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, by tuning eps based on the cosine distance graph, the number of points incorrectly classified as noise (-1) was reduced, and more coherent, distinct mini-clusters were formed for highly specific content niches (e.g., Stand-up comedy specials)."
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I considered the Silhouette Score because it evaluates both cohesion (how close items in a cluster are) and separation (how far apart different clusters are). For Netflix, high cohesion means reliable content recommendations (positive user experience, lower churn rate), while high separation ensures diverse categories on the homepage, catering to varying user moods."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose Model 1: K-Means Clustering.\n",
        "\n",
        "Why: It provided the most balanced and interpretable clusters compared to DBSCAN (which struggled with text sparsity and noise) and Hierarchical clustering (which is computationally expensive for large datasets). K-Means allows for straightforward cluster centroids, making it highly efficient to map new content to existing groups in a live production environment."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because K-Means is unsupervised, standard tools like SHAP are less effective. Instead, we explain the model by extracting the top TF-IDF features (words) for each cluster centroid."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File\n",
        "import joblib\n",
        "\n",
        "# Saving the K-Means model, TF-IDF vectorizer, StandardScaler, and PCA model\n",
        "joblib.dump(kmeans, 'netflix_kmeans_model.pkl')\n",
        "joblib.dump(tfidf, 'netflix_tfidf_vectorizer.pkl')\n",
        "joblib.dump(scaler, 'netflix_scaler.pkl')\n",
        "joblib.dump(pca, 'netflix_pca.pkl')\n",
        "\n",
        "print(\"Model, Vectorizer, Scaler, and PCA successfully saved to disk.\")"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data.\n",
        "loaded_kmeans = joblib.load('netflix_kmeans_model.pkl')\n",
        "loaded_tfidf = joblib.load('netflix_tfidf_vectorizer.pkl')\n",
        "loaded_scaler = joblib.load('netflix_scaler.pkl')\n",
        "loaded_pca = joblib.load('netflix_pca.pkl')\n",
        "\n",
        "# Unseen Data (e.g., A brand new sci-fi movie description)\n",
        "new_movie_description = [\"A futuristic sci-fi action movie set in space with aliens and lasers.\"]\n",
        "\n",
        "# 1. Transform using the loaded TF-IDF vectorizer\n",
        "new_movie_vectorized_tfidf = loaded_tfidf.transform(new_movie_description)\n",
        "\n",
        "# 2. Convert to dense array and scale using the loaded scaler\n",
        "new_movie_vectorized_scaled = loaded_scaler.transform(new_movie_vectorized_tfidf.toarray())\n",
        "\n",
        "# 3. Apply PCA using the loaded PCA model\n",
        "new_movie_vectorized_pca = loaded_pca.transform(new_movie_vectorized_scaled)\n",
        "\n",
        "# Predict using the loaded K-Means model\n",
        "predicted_cluster = loaded_kmeans.predict(new_movie_vectorized_pca)\n",
        "\n",
        "print(f\"Sanity Check passed! The new movie was assigned to Cluster: {predicted_cluster[0]}\")"
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this project, we successfully developed an unsupervised machine learning pipeline to cluster Netflix Movies and TV Shows, aiming to enhance the platform's content recommendation engine and improve user experience.\n",
        "\n",
        "Here is a summary of our key steps and findings:\n",
        "\n",
        "1. Data Exploration and Preprocessing:\n",
        "\n",
        "We handled missing values in critical columns like director, cast, and country to retain as much data as possible.\n",
        "\n",
        "Through comprehensive Exploratory Data Analysis (EDA), we uncovered key trends, such as the platform's historical dominance of Movies over TV Shows, while noting the rapid recent expansion of episodic content. We also explored the distribution of content across different countries and ratings.\n",
        "\n",
        "We engineered a rich textual feature by combining the description, listed_in (genre), cast, and director columns. This text was thoroughly cleaned (removing stopwords and punctuation) and transformed into numerical representations using TF-IDF Vectorization.\n",
        "\n",
        "2. Model Implementation and Evaluation:\n",
        "\n",
        "We implemented and compared three distinct clustering algorithms: K-Means, Agglomerative Hierarchical Clustering, and DBSCAN.\n",
        "\n",
        "K-Means Clustering emerged as the best-performing model. Using the Elbow Method and Silhouette Analysis, we determined that K=6 was the optimal number of clusters, effectively balancing distinct content groupings without over-fragmenting the catalog.\n",
        "\n",
        "Hierarchical clustering provided excellent visual taxonomies via dendrograms, while DBSCAN struggled slightly with the high-dimensional, sparse nature of our TF-IDF text data, often categorizing too many points as noise.\n",
        "\n",
        "3. Business Impact:\n",
        "\n",
        "The final K-Means model successfully partitioned the Netflix catalog into 6 distinct, contextually meaningful clusters based on textual similarity.\n",
        "\n",
        "By understanding these intrinsic content groupings, Netflix can significantly improve its recommendation systems. Recommending items from the same cluster as a user's previously watched content increases the likelihood of user engagement, maximizes watch time, and ultimately reduces subscriber churn.\n",
        "\n",
        "4. Future Scope:\n",
        "\n",
        "The final model and TF-IDF vectorizer were successfully serialized using joblib, making the pipeline fully ready for deployment in a live production environment.\n",
        "\n",
        "Future iterations of this project could explore advanced Natural Language Processing (NLP) techniques, such as Word2Vec or BERT embeddings, to capture deeper semantic meanings in the text. Additionally, integrating user viewing history to create a hybrid recommendation engine (combining these content clusters with collaborative filtering) would further personalize the user experience.\n",
        "\n",
        "This completes the end-to-end lifecycle of the Netflix Clustering project, from raw data to a deployment-ready machine learning model."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}